services:
  whisperx-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: whisperx-api
    restart: unless-stopped
    
    # GPU access - requires NVIDIA Container Toolkit
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use 'all' for all GPUs
              capabilities: [gpu]
    
    ports:
      - "9050:8000"
    
    environment:
      # API Settings
      - APP_NAME=WhisperX Transcription API
      - APP_VERSION=1.0.0
      - DEBUG=false
      
      # API Keys (use secrets in production)
      - API_KEYS=${API_KEYS:-your-secure-api-key}
      
      # HuggingFace Token (required for diarization)
      - HF_TOKEN=${HF_TOKEN}
      
      # WhisperX Settings
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v2}
      - DEVICE=cuda
      - COMPUTE_TYPE=${COMPUTE_TYPE:-float16}
      - BATCH_SIZE=${BATCH_SIZE:-16}
      
      # Storage
      - UPLOAD_DIR=/app/uploads
      - MAX_FILE_SIZE_MB=${MAX_FILE_SIZE_MB:-500}
      
      # Worker Settings
      - MAX_CONCURRENT_TASKS=${MAX_CONCURRENT_TASKS:-2}
      - TASK_TIMEOUT_SECONDS=${TASK_TIMEOUT_SECONDS:-3600}
      
      # Cleanup
      - CLEANUP_AFTER_HOURS=${CLEANUP_AFTER_HOURS:-24}
    
    volumes:
      # Persist uploaded files
      - whisperx-uploads:/app/uploads
      
      # Cache HuggingFace models (speeds up container restarts)
      - whisperx-cache:/home/appuser/.cache
    
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8000/health', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Models take time to load
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  whisperx-uploads:
    driver: local
  whisperx-cache:
    driver: local
